paulo 29/06:
* porque? como a complexidade estrutural pode ser interpretada (explicada) no dominio
  de aplicacao de analise estatica de codigo-fonte?
* como? 

feedback chris 21/06:
* caracterizar outras dimensões das ferramentas, fudamentar isso e analisar as ferramentas
* qual a motivacao para calcular distancia Bayesiana, o uso, serve para decidir adotar
* qual a motivacao da manutenabilidade, se uma empresa quer adotar mas tem uma livre q n faz
  tudo q ela quer mas algum faz 70% e tem uma boa manutenabilidade, ele pode usar isso pra
  decisão, ...
* Darci, tese de terceiro, explicar melhor como vou usar SC
* talvez n de trabalho, pegar um algoritmo de clusterizacao, algo simples, pega
  o codigo e pelas dependencias clusteriza
* existe uma arquitetura ideal, a arquitetura das ferramentas podem srr avaliadas
  se seguem, por exemplo, (objetivo: coletar para calcular ou propor intervalos de referencia)
  (recuperar arquitetura das ferramentas para propor uma arquitetura de referencia)
* mojo eh uma ferramenta que calcula a distancia entre os 2, pegar um algoritmo
  de clusterizacao pronto (bunch ou acedecee)
  (posso pedir para matheus)
* merece uma explorada nisso porque no geral, consigo avaliar a aderencia de uma ferramenta a
  partir de uma arquitetura de referencia (isto eh para comport a parte de analisa da complexidade estrutural)
* considerar possibilidade recuperar arquitetura
* o objetivo eh apenas ter mais um componente, nao preciso entrar a fundo nem justificar
  a escolha de um algoritmo X ou Y
* Mover analizo para metodologia e voltar a falar no estudo de caso, em cada ponto
  eu falo apenas o que eh necessario
* a hipotese 3 exige melhorar a fundamentacao sobre complexidade estrutural

paulo 22/06:
* o calculo bayesiano eh para prova a H3, enfatizar a diferenca da qualidade
  de codigo da industria e dos artigos

* Feedback de paulo sobre os dados:
  * CBO claramente tem um comportamento diferente das demais, geralmente
    o crescimento é exponencial ou calda longa, mas CBO não tem mostrado
    este comportamento, as vezes é calda loga inertida
  * Na qualificação posso mostrar todas as métricas e na discussão
    irei selecionar algumas (possivelmente CBO, LCOM4 e SC)
  * Fácil notar só olhando a tabela que no percentil 75% (muito frequente)
    os valores estão bons, bons significa próximo dos valores teóricos ideais
  * Os valores dos percentis 90% e acima são pontos fora da curva, n podemos
    usar estes valores como alerta para indicar que o projeto está ruim
  * O crescimento reflete o mesmo comportamento dos estudos de paulo e
    kanashiro

* Feedback Chris/Paulo:
  * Na fund teorica cito arquitetura da ferramenta de analise estatica,
    irei discutir isto na metodologia/analise?
  * A caracterização da SC irá indicar que a qualidade n é ruim necessariamente
    mas que é uma caractaristica natural deste domínio, e isto irá guiar
    como proceder para evoluir e manter tais ferramentas
  * Tese de Paulo 4.1 e 4.3 (percentis, a média pode nos enganar)
  * A SC é mais alta neste domínio de aplicação (hipótese)
  * Ao menos objetivo geral, especifico, questao e hipoteses, contribuicoes, etc...
  * ... porque? para quem quiser manter/evoluir uma ferramenta de analise estatica
    entender que uma SC X ou Y para este dominio n eh ruim, mas é uma caracteristica natural

Métricas de design ou métricas de código-fonte são aquelas voltadas à medir
artefatos de código-fonte e podem ser classificados entre métricas procedurais
ou orientadas à objetos \cite{Mohamed1994}.

\chapter{Métricas de design de código-fonte}

(ver trabalho de Arthur Del Spot e Tese de Paulo, SC (focar) conforme Terceiro)

\chapter{Metodologia}

%explicar:
%* revisão estruturada
%* análise de distribuição dos valores das métricas
%
%(ver TCC de Ronaldo e Kanashiro)
%( falar que média não serve, distribuição é a solução... etc )

\section{Exemplo de uso}

Por fim, os valores de referência encontradas serão documentados e servirão de
base para criar um guia de sugestões para refatoração de ferramentas de
análise estática. Tomaremos como exemplo de uso a própria ferramenta Analizo,
onde iremos calcular suas métricas e a partir de uma comparação com os
valores de referência indicaremos refatorações que façam a ferramenta de
aproximar dos valores de referência.

\chapter{Cronograma}


\begin{table}[h]
  \caption{Cronograma até a qualificação}
  \centering
  \begin{tabular}{| l | l |}
    \hline
    Data          & Atividade \\
    \hline
    20 Junho      & texto final para os orientadores \\
    22/23 Junho   & feedback final dos orientadores \\
    23 à 27 Junho & correções finais \\
    27 Junho      & envio para banca \\
    27 Junho à 04 Julho & compilação dos últimos resultados, inclusive o que não entraram no texto da quali (mas que podem ser apresentados na defesa da Quali) \\
    6/7/8 Julho   & (a depender do dia da sua viagem para SSA): Prévia/ensaio da apresentação no LAPPIS/UnB. \\
    11 Julho      & Qualificação, na UFBA \\
    \hline
  \end{tabular}
  \label{artigos-do-scam}
\end{table}

\chapter{Análise}

\section{Resultados preliminares}\label{resultados}

\section{Desafios da análise estática}

Principal 2: Less is More. Does this movement that
started in graphic design speak to source-code analysis?
Any programmer who has set out to write an analysis tool
confronts significant obstacles. First, simple parsing is not
as simple as it should be. For example, C++ is not LR(n)
(for any n!). Furthermore, the presence of casting, pointer
arithmetic, dynamic class loading, reflection, and the like,
complicate semantic analysis. This leads to the question
“how good a tool would I need to create for programmers to
use a ‘simple’ language?” Language design seems headed
in the opposite direction (Fortran “9x” is an excellent ex-
ample). Against an increasing need for higher precision
source-code analysis, modern languages increasingly re-
quire tools to handle only partially known behavior (in the
case of Java this is caused by features such as generics, user-
defined types, plug-in components, reflection, and dynamic
class loading). These features increase flexibility at run-
time, but compromise static analysis. In considering each
of the following research areas, consider also the role that
analysis quality should play in the language design debate.

Until alternate formalisms come into common usage,
source code will continue to be definitive in describing pro-
gram behavior. Based on the past 40 years, programming
languages can be expected to continue to incorporate new
features that complicate program semantics. Given its cen-
tral role in software engineering, source code and source-
code analysis will remain “hot topics” and thus the focus
of intense research activity into the foreseeable future. To
ensure future progress including necessary, but unforeseen
breakthroughs, it is important for future source-code anal-
ysis research to continue to investigate a wide diversity of
ideas.

%Many of these tools
%are seeking the same kind of programming errors, but, to
%date, only a few tools have been compared with each other.
%In this paper, we will look at what the static code analyzers
%can do and we will try to present a systematic tree of
%features and assets of static code analysis tools. Finally, we
%will attempt to compare a few tools for static analysis of
%code in our newly developed systematic tree.
%
%este artigo faz uma caracterizacao muito interessante de ferramentas de
%análise estática, cria categorias para caracterizar, muito bom! é algo do que
%eu gostaria de fazer originalmente, posso guardar isso pro futuro e usar as
%mesmas caracteristicas para aplicar em outro grupo de ferramentas.
%
%(/home/joenio/Downloads/Taxonomy of Static Code Analysis Tools.pdf)


 Compara três ferramentas lider de mercado em
2006/7: PolySpace Verifier, Coverity Prevent e Klocwork K7. Identificar
funcionalidades sifnificativas de ferramentas de analise estática, mas  que
não são fornecidas por compiladores normais. Por análise estática dizemos
métodos para racionalizar sobre propriedades de runtime do código de um
programa sem execurar. Ferramentas de análise estática para detecção de
defeitos em runtime e vulnerabilidades de segurança podem ser categorizados
em: String e combinação de padrões, Análise Unsound dataflow, Análise sound
dataflow \cite{Emanuelsson2008}.

As ferramentas mais comumente usadas
podem ser classificadas em diversas categorias: tecnologia, disponibilidade de
regras (rules), linguagens suportadas, extensibilidade, e muitas outras.
Apenas poucas tem sido comparadas umas contra outras. Estas ferramentas
procurar error que compiladores não olham. Analisadores estático de código
trabalham de diferentes formas, podem operar no código-fonte, outros em código
intermediário. Outra diferença é o fato de diferentes analisadores operarem em
diferentes linguagens, como C, C++, C# ou Java. Alguns exemplos de problemas
que podem ser reconhecidos por analisador estático de código: Syntactic
problems, unreachable source code, undeclared variables, non-initialized
variables, non used functions and procedures, variables used before
initialization, non use of values from functions, wrong use of pointers.

Categorias para caracterizar análise estática de código:

Entrada - quais tipos de arquivos podem ser carregados na ferramenta:

Código-fonte - arquivos de código texto podem ser carregados
Byte code - arquivos com Java Byte Code ou Microsoft
Linguagem intermediária (MSIL) pode ser carregada

Linguagens suportadas - quais linguagens de programação a ferramenta suporta

.NET - todas as linguagens compiladas em bibliotecas ou programas no framework .NET
VB .NET - suporta VB.NET
C# - suporta C#
Java - suporta linguagem de programação Java
C, C++ - suporta linguagem de programação C ou C++

Tecnologia - quais tecnologias são usadas para procurar erros no código

Dataflow - busca por erros com dataflow
Sintaxe - busca por errors de sintaxe e correctness
Prova de teoremas - procurar erros em provar diferentes teoremas
Verificação de modelos - procurar erros com verificação de modelo

Regras - conjunto de regras, quais são suportadas por diferentes código estático

Estilo - inspecina a aparecencia do código fonte
Naming - checa se as varáveis são nomeadas corretamente (ortografia, padroes de nomenclatura, ...)
Geral - regras gerais de analise estática de código
Comcorrencia - erros com execução de código de concorrente
Exceções - erros lançando ou não exceções
Performance - erros de performance das aplicações
Interoperabilidade - erros de comportamento comum
Segurança - erros que podem impactar na segurança da aplicação
SQL - procurar por "SQL injections" e outros erros de SQL
Buffer overflow - erros de segurança, que explorar buffer overflow
Manutenabilidade - regras para melhor manutenabilidade da aplicação

Configurabilidade - abilidade de configurar a ferramenta

Documento texto - comfiguração é feita via documento texto
XML - configuração pe feita por documento XML
GUI - configuraçãi é feita via interface gráfica
Ruleset - ferramenta pode ligar/desligar conjunto de regras

Extensibility – if tool can be extended with own rules

Possible – it is possible to extend
Not Possible – it is not possible to extend

Availability – in what way is tool available

Open Source – tool is free and source code is available
Free – tool is free, but source code is not available
Commercial – tool is available for payment

User experience – in what way can tool be used in what is offered to us

Environment integration – how is tool integrated with working environment
Automatic locating errors in code – when tool finds an error, it can put as at the location of the error
Extensive help on faults – if tool gives you help on resolving errors
User interface – availability of user interface
Command Line – it can be run from command line prompt
GUI – tool can be run from GUI interface

10. Output – presentation of the results from tool

Text file – tool can present results in text file
List – tool can present results in custom user interface control in GUI
XML file – tool can present results in XML data
HTML file – tool can present results in HTML data

(Figure 1 Taxonomy of static code analysis tools)

caracterizou as seguintes ferramentas: CheckStyle, FindBugs, Gendarme, StyleCop

\cite{Novak2010}

\cite{Novak2010} em seu artigo ``Taxonomy of Static Code Analysis Tools''
constrói uma taxonomia para ferramentas de análise estática a partir do estudo
de quatro ferramentas de análise estática populares.


Análise estática é eficaz e complementar a testes dinamicos. Assim seu uso é
comendado no contexto da maioria dos softwares criticos. A análise menos
profunda métodos que não requerem extensiva informacao de design pode ser
usado pela maioria dos softwares. Nao existem padroes apropriados. Isto
implica que a especificação da análise estática a ser realizada requer cuidado
e esforço. Igualmente, o potencial beneficio desssas analises nem rempre pode
ser facilmente apreciadas. A profundidade e natureza de uma analise precisa
ser especificada. Para dizer que analise estatica foi realizada eh apenas uma
forma de dizer que o software foi testado. Entretando ao menos estes 2
problemas (profundiade e natureza) precisa ser especificada ate certo ponto. O
prerequisito para a validade de diferente formas de analise precisa ser
reconhecida, para permitir avaliacao objetiva da validade de uma particular
aplicacao de uma analise tecnica. A linguagem de entrada influencia a
profundidade da analise estatica pode ser feita facilmente. Linguagens que sao
esencialmente dinamicas, como C, sao mais dificeis de analisar que linguagens,
como Ada, que incluem tipagem forte e restrições de alcance. Entretando, a
natureza da linguagem de entrada precisa ser levada em conta na especificacao
da analises a ser realizada. O uso de mais rigorosos linguagens 'engenharia de
software' e conjuntos apropriados podem permitir menos profunda formas de
analise para ser classificado em regras da linguagem, entao nós experamos que
separar passos de controles e fluxo de dados analise nao sao mais requeridos.

\cite{Wichmann1995}

Para entender as
limitacoes destas tecnicas que estas ferramentas usam, eh importante entender
metricas usadas para avaliar sua performance. Recall é a medida da habilidade
da ferramenta de encontrar problemas reais. Precisão mede a habilidade da
ferramenta de excluir falso positivos. Performance, apesar de n ser
formalmente definido, esta eh uma medida do quando recurso computacional
precisa para gerar seus resultados. Estas 3 metricas geralmente operam em
oposicao entre elas.  Ferramenta de analise estatica avancadas oferecem muito
para melhorar a qualidade de software. As melhores ferramentas sao faceis de
integrar no ciclo de desenvolvimento, e podem produzir alta qualidade de
resultados rapidamente sem requerer esforço adicional de engenharia
\cite{Anderson2008}.

Existem muitas tecnicas sob o guarda chuva do termo analise estatica de
código, e estes podem ser caracterizados pela sua natureza e profundiade.
Natureza refere ao conjunto de objetivos da analise e pode estar preocupado
com propriedades especificas como portabiliade. Profundiade significa
profundidade analitica da tecnica.

\cite{German2003}

The expressive power of programming languages makes
analysis even harder. Analysis routines must be explicitly
developed to handle coding complexities, such as loops,
conditional control flow, arrays, different variable types,
interprocedural function calls, and aliasing. In practice no
tool handles all possible code constructs.

\cite{Black2007}

The purpose of this project is to evaluate existing source code analysis tools, according to metrics
prioritizing ease of use and “quick gains” over more efficient but more complex solutions.

Categories of software analysis
tools can be established according to various criteria. One of the most relevant and interesting
criteria is whether a process is “manual” or “automatic”. Manual software improvement processes
involve a developer or a quality analysis expert to actively evaluate the piece of software, make use
of their previously acquired knowledge to understand it and identify its shortcomings. Automatic
methods consist of a tool that performs a predetermined series of tests on the piece of software it is
given and produces a report of its analysis.

avaliou uma série de ferramentas usando metricas de qualidade externa: Installation
Configuration
Support
Reports
Errors found
Handles projects?

Astree
BOON
CCA
HP Code A
cppcheck
CQual
Csur
Flawfinder
ITS4
Smatch
Splint
RATS

CodePro
FindBugs
Hammurapi
JCSC
IBM
PMD
QJPro

B::Lint
Perl::Critic
RATS
Taint Mode

Sandcat
Pixy
RATS

PEP8
PyChecker
Pylint
RATS

\cite{Hofer2010}

The BCS SIGIST defines static analysis of source code
as the “analysis of a program carried out without executing
the program” [24]. This definition emphasizes the contrast
to dynamic analysis, where the behaviour of program is
observed while it is executed.

Static analysis is not exclusively used for finding security
problems. Applications can also be found, for instance, in
compiler optimization or improvement of general code quality.

In general, static analysis problems are undecidable [11].
Thus, no analysis can be sound and complete. This means that
a static analysis program is either unable to reliably detect all
targeted problems or is prone to false positives, i.e., it reports
findings which turn out to be wrong on closer examination. A
further discussion about the implications and necessary trade-
offs is given in [28] and [2]. In practice, most tools expose
both variants of erroneous behaviour.
Available static analysis for security programs exist in the
form of freeware [26] [25] [22], academic prototypes [5] [2]
[12] [23], company-internal tools (e.g., Microsoft’s Prefast [7]
and Prefix [13]), and commercial products (see [18] for an
overview).

Evaluation criteria
Quality of the analysis.
Implemented trade-offs between precision and scalability.
Set of known vulnerability types.
Usability of the tool.

evaluating the quality of a static analysis tool for security

\cite{Johns2011}

This study evaluated three
ASA tools: IntelliJ ® IDEA, Jlint, and FindBugs.

In conclusion, our results indicate that the ASA tools that
we evaluated are not effective in detecting the faults that
are eventually reported. However, two of the tools can find
program structures that are candidates for refactoring. The
cost of using these tools may be prohibitive due to the large
number of false positives.
Our research focused on the use of three ASA tools for
Java programs. In future work, we plan to examine cod-
ing concerns reported by additional ASA tools, and study
software written in other programming languages such as
C/C++. Also, we plan to identify the types of faults that
ASA tools can detect more successfully.

\cite{Wedyan2009}

Tese onde avlia 4 ferramentas em relaçao a buffer overflow \cite{Kratkiewicz2005}

Static analysis tools can handle large-scale software and find
thousands of defects. But do they improve software security?
We evaluate the effect of static analysis tool use on software
security in open source projects. We measure security by
vulnerability reports in the National Vulnerability Database.

But does their use improve software security?

The goal of this
study is to evaluate the effect of tools on software security.

A number of studies have compared different static analysis tools
for finding security defects, e.g.

\cite{Okun2007}

Static analysis (SA) tools are being used for early detection
of software defects.

This paper presents the evaluation of four static
analysis tools and their capabilities to detect Java concur-
rency bugs and bug patterns. The tools, i.e., Coverity Pre-
vent, Jtest, FindBugs, and Jlint, are evaluated using concur-
rent benchmark programs and a collection of multithreaded
bug patterns.

Are commercial SA tools better than open source
SA tools in detecting Java concurrency defects?

In addition, we found several studies evaluating static anal-
ysis tools from different perspectives other than detecting
concurrency bugs. A study by Painchaud et al. [18] evalu-
ated four commercial and seven open source static analysis
tools. Their study also recommended a six steps methodol-
ogy to assess the software quality.

F. Wedyan et al. [24] evaluated the usefulness of automated
static analysis tools for Java program. They evaluate the
effectiveness of static analysis tools for detecting defects and
identifying code refactoring modifications.

[18] F. Painchaud, R. Carbone, and D. Valcartier. Java
software verification tools: Evaluation and
recommended methodology. Technical memorandum
TM 2005-226, Defence R&D Canada, 2007.

[24] F. Wedyan, D. Alrmuny, and J. M. Bieman. The
effectiveness of automated static analysis tools for
fault detection and refactoring prediction. In Proc. of
the 2nd Int’l Conf. on Software Testing, Verification,
and Validation (ICST), pages 141–150, April 2009.

\cite{Al2010}

Using static analysis tools can detect software
vulnerabilities, which is important for improving the security
of software. Static analysis technology has developed rapidly,
but the comparison and evaluation of static analysis
techniques and tools are not much. This paper focuses on
software vulnerability static analysis techniques and tools.
First we discuss the commonly-used static analysis techniques
and tools, and compare these tools in a technical perspective,
and then we analyze the characteristics of these tools through
the experiment, finally, combining dynamic analysis, we
propose an efficient software vulnerability detection method.

Static analysis is simple, fast and can be effective to find
bugs in the code. Therefore, many software analysis tools
are designed and achieved by static analysis technology. The
first static analysis tool is FlexeLint in 1980s years; it used
pattern matching method to identify gaps. In recent years, a
number of complex and powerful static analysis tools began
to appear.

But few
studies have analyzed and evaluated different static analysis
tools, so it is quite negative to promote and apply static
analysis tools.

Our study focused on
identifying static analysis functionality provided by the tools
and surveying the underlying supporting technology. First
we discuss commonly-used static analysis techniques and
tools, and then from a technical point of view we compare
these tools, after that through the experiment we analyze the
characteristics of these tools,

ITS4
SPLINT
UNO
Checkstyle
ESC / Java
FindBugs
PMD

\cite{Li2010}

These tools scan software for bug patterns or
show that the software is free from a particular class of defects.

Among others, we have developed the following five tools.
FindBugs FindBugs [4] is a static analysis tool that finds coding
mistakes or defects in Java programs. The approach taken by the
FindBugs project is to start with real bugs in real software, abstract

a bug pattern from those bugs, and devise the simplest possible
detector that can effectively find that bug pattern, as evaluated by
trying the proposed detector on test cases for that bug pattern as
well as tens of millions of lines of production software. FindBugs
now finds more than 250 bug patterns. Some bug detectors, such
as those reporting null pointer errors [5], reflect lots of work over
a long period of time, and find lots of errors. But there is also a
long tail of bug detectors/patterns: many patterns only occur a few
times per million lines of code, but in total find a substantial number
of defects. Such detectors are frequently conceived, implemented,
evaluated and tuned within the space of a few hours.
FindBugs can be run within multiple environments, and also
supports historical tracking [8] so that the persistence of defects
across successive builds of a software project can be captured.
FindBugs has been downloaded more than 300,000 times, and
is now in use by many large companies and open source efforts.
This provides an opportunity to study open code bases and see how
the defects identified by FindBugs change over time, both in cases
where FindBugs is in use and in cases where it is not. FindBugs
also provides an excellent delivery vehicle for allowing new defect
detection research ideas to be quickly disseminated.

em trabalhos futuros faz perguntas muito importantes do meu ponto de vista!

What do we need to do to make a tool useful for a developer?
Much academic research has focused on the fundamental algo-
rithmic challenges of such tools rather than their usability. We
need to study, both scientifically and anecdotally, what design
choices make tools better or worse in practice.

How can we help programmers understand the output of a static
analysis tool? We need better ways to present static analysis
results in terms of the abstractions the developer has in mind.
Just as we would like to have standard tool front-ends, can we
develop standard back-ends for presenting analysis results?

\cite{Foster2007}

In this paper we compare three static code analysis tools.
The tools represent three different approaches in the field
of static analysis: Fortify SCA is a non-annotation based
heuristic analyzer, Splint represents an annotation based
heuristic analyzer, and Frama-C an annotation based cor-
rect analyzer. The tools are compared by analysing their
performance when checking a demonstration code with in-
tentionally implemented errors.

This is the area where the static analysis
tools come into play: they aid the auditor by pointing to
possible bugs and failures in coding practice, thus saving
time and effort

tools: SCA, Splint, Frama-C, ACSL

Static code analysis tools can be very expensive, have
steep learning curves, be too old for any serious work or
need annotations to be written to libraries. They should be
carefully studied before selecting which one to use in a soft-
ware project.

\cite{Mantere2009}

Context: Static analysis tools are used to discover security vulnerabilities in source code. They suffer from
false negatives and false positives. A false positive is a reported vulnerability in a program that is not really
a security problem. A false negative is a vulnerability in the code which is not detected by the tool.

We propose some recommendations for improving the reliability
and usefulness of static analysis tools and the process of benchmarking.

Static analysis is not the only technique to analyze a system
against the presence of security problems. Dynamic testing meth-
ods are used to discover vulnerabilities using different techniques
during the execution of the system being studied, as shown in
Aggarwal and Jalote [21], SAMATE [6], Petukhov and Kozlov [22].
At the present state of technology, static and dynamic tools appear
to be clearly complementary, as also suggested for example in
Zheng et al. [11] when comparing static analysis tools (including
K8-Insight, one of the tools analyzed in our study) with dynamic
testing tools.

This
work is also complete in code complexity variety. The code complex-
ity concept refers to variants in the code, which can lead to a con-
crete vulnerability: directly in main body, in a called function, in a
loop, array, pointer, etc. Tools may not detect a concrete vulnera-
bility when the code complexity changes, therefore a benchmark
should include code complexity diversity for each vulnerability.

Some other works analyze static source code tools against dif-
ferent programs. For example, Rutar et al. in [35] analyzed five dif-
ferent tools (Bandera, ESC/Java 2, FindBugs, JLint, and PMD) against
a well known number of Java programs. Their results show that the
tools find non-overlapping vulnerabilities and authors proposed a
meta-tool to allow developers to identify the different classes of
vulnerabilities.

Wagner et al. [36] compared three different static analysis
tools: FindBugs, PMD and QJ Pro. They were executed against five
industrial projects and one development project from a university
environment which were in use or in the final testing phase. Their
results showed again very different results for the three tools.

As Fig. 2 shows, the code is transformed using a combination of
different techniques as lexical and semantic analysis, abstract syn-
tax and parsing. The program model is analyzed using intraproce-
dural (local) analysis for examination of individual functions and
interprocedural (global) analysis for the interaction between func-
tions, using tracking control flow and data flow, pointer aliasing,
etc.

Different schemes exist to categorize static security analysis
tools according to McGraw [42]. The classification can be made
attending to the languages they understand or to the kind of vul-
nerabilities searched. Another criterion is the length of the code
to be analyzed, only small applications or big projects. We consider
that the most relevant taxonomy for the purpose of our work is
shown in Chess and West [4]. This taxonomy refers to the general
purpose of the tools and differentiates between the following
types:

Style checking
These tools (for example, the lint tool in UNIX) usually enforce a
more selective and more superficial set of rules than a type check-
er. They perform checks based on lexical and syntactic analysis
(earlier generation of static analysis) to discover problems as
inconsistencies in function calls, return values in some places
and not in others, functions called with varying numbers of argu-
ments, function calls that pass arguments of other types. This anal-
ysis has obviously limitations, when compared with other types

(Fig. 2. Static analysis tools process.)

Program understanding
These tools are designed for helping users to make sense of a
project code. They are included in many Integrated Development
Environments and are designed to help programmers to gain in-
sight into the way a program works. They help the reviewer who
performs security analysis to understand the code and discover
vulnerabilities but, in any case, this is a manual review of all code
and it is time consuming. An example is the Fujaba Tool [43].

Program verification and property checking
These tools accept a specification and a body of code and then
attempt to prove that the code is a correct implementation of the
specification. Some examples are CBMC [44], Polyspace tools [25],
Codesonar [45], or Satabs [46].
Finally, this classification refers to the most modern and sophis-
ticated tools. We use the names given to these static analysis tools
by Chess and West in [4]: ‘‘bug finding’’ tools and ‘‘security review’’
tools. We consider more in detail both types of tools because of
their application for the security analysis of projects of hundreds
of thousands or millions of lines of code.

3.3. ‘‘Bug finding’’ static analysis tools

A number of these tools is available as, for example, FindBugs
[47], a general tool for identifying vulnerabilities in Java code
or Prevent, a commercial tool from Coverity [32], available for
C/C++, Java, J2EE and C#. Also Microsoft has its own tool, Microsoft
Visual Studio 2005 with the ‘‘/analyze’’ option, sometimes known
as Prefast [48], which is able to check common coding error in C
and C++ languages. Finally Klocwork offers Insight [33], available

also for C/C++, Java, J2EE and C #, a product suite that allows graph-
ical exploration of programs with hundreds of thousands or mil-
lions of lines of code.

3.4. ‘‘Security review’’ static analysis tools

These tools combine many of the techniques of the previous
tools but with the much stricter goal of identifying specific security
vulnerabilities. As claimed by their vendors, their design imple-
ments in fact a combination of property checkers and ‘‘bug finding’’
class of tools techniques, because many security properties can be
expressed briefly as program properties. The designers of these
tools prefer the cautious side of the balance between false positives
and negatives. These tools prefer to show many points in the code
that should be manually reviewed after the execution of the tool,
producing more false positives when compared with ‘‘bug finding’’
tools.
Two of the most relevant ‘‘security review’’ tools are Ounce 6,
now a product from IBM [49] and SCA (Source Code Analyzer) from
Fortify Software [50], now an HP company. SCA is able to analyze
code in different languages, as C/C++, C#, ASP NET, VB.NET, COBOL,
CFML, HTML, Java, JavaScript, AJAX, JSP, PHP, PL/SQL, Python, Visual
Basic, VBScript and XML.

tools: Satabs, CBMC

\cite{Diaz2013}

we take a look at some of the different categories of static analysis tools

Type checking
Style checking
Program understanding
Program verification
Property checking
Bug finding
Security review

compara algumas ferramentas para bug finding em Java \cite{Rutar2004}

For style checking Java, we
like the open source program PMD (http://pmd.sourceforge.net) because it
makes it easy to choose the style rules you’d like to enforce and almost as
easy to add your own rules. PMD also offers some rudimentary bug detec-
tion capability. Parasoft (http://www.parasoft.com) sells a combination bug
finder/style checker for Java, C, and C++.

Program understanding
The open source Fujaba tool suite (http://wwwcs.uni-paderborn.de/cs/
fujaba/), pictured in Figure 2.1, enables a developer to move back and forth
between UML diagrams and Java source code. In some cases, Fujaba can
also infer design patterns from the source code it reads.
CAST Systems (http://www.castsoftware.com) focuses on cataloging and
exploring large software systems.

Program Verification and Property Checking
Praxis High Integrity Systems (http://www.praxis-his.com) offers a
commercial program verification tool for a subset of the Ada programming
language [Barnes, 2003]. Escher Technologies (http://www.eschertech.com)
has its own programming language that can be compiled into C++ or Java.
Numerous university research projects exist in both the program verification
and property checking realm; we discuss many of them in Chapter 4. Poly-
space (http://www.polyspace.com) and Grammatech (http://www.gramat-
ech.com) both sell property checking tools.

Bug finding
We think FindBugs does an excellent job of identifying bugs in Java code.
Coverity makes a bug finder for C and C++ (http://www.coverity.com).
Microsoft’s Visual Studio 2005 includes the \analyze option (sometimes
called Prefast) that checks for common coding errors in C and C++.
Klocwork (http://www.klocwork.com) offers a combination program
understanding and bug finding static analysis tool that enables graphical
exploration of large programs.

Security Review
Fortify Software (http://www.fortify.com) and Ounce Labs
(http://www.ouncelabs.com) make static analysis tools that specifically
A third company, Secure Software, sold a static analysis tool aimed
at security, but in early 2007, Fortify acquired Secure’s intellectual property.
Go Fortify!

\cite{Chess2007}




%Control and Dataflow Analysis
%
%One representation that is widely used in formal methods for optimizing and verifying source code
%is that of Control Flow Graphs (CFG). In a CFG, each node represents a basic bloc of code, i.e.
%lines of code that will not be executed independently during normal program execution. In other
%words, the nodes do not contain any jumps or labels (jump targets). The jumps are represented by
%directed edges. Usually CFG implementations have two special nodes: the “entry point”, which is
%the starting point of the execution; and the “exit point”, where the execution exits the graph.

+%Another commonly used graph is the Dependence Graph, introduced in the context of work
+%with parallelizing and highly optimizing compilers [FOW87], where vertices represent the state-
+%ments and predicates of a program. These graphs have been used in other analyses [HRB88,
+%HR92]. 
+%
+%Other sorts of graphs, also referred in the literature, include Dynamic Call Graphs [QH04,
+%PV06] (is intended to record an execution of a program) and XTA graphs built in support of
+%dynamic reachibility-based interprocedural analysis [QH04].
+

    \item Trace Flow Graph (TFG)
    \item Value Dependence Graph (VDG)



%Tools Available
%There are a number of static code analy-
%sis tools available. They offer different
%depths of analysis, and some will only
%operate on a few languages. Most of
%them run on uncompiled source code
%and first translate to an intermediate lan-
%guage, which the analysis tool itself can
%read.
%The time taken for the tool to analyze
%the code may be only a small fraction of
%the time taken to carry out static analysis
%of the code. Many tools produce reams
%of data that must be laboriously analyzed
%and processed; staff requires skill and a
%lot of training.




%Esta pode ser uma atividade
%complexa, como variaveis globais podem ser acessadas de qualquer lugar. Esta
%analise tb pode detectar outras anomalias como multiplas gravacoes sem
%leitura.
%Com o uso de Control
%Flow Graph a análise de fluxo de dados determina se um valor no programa é
%atribuído para uma possível vulnerabilidade de variáveis.

%\item \textit{Control Flow Analysis.}
%
%CFA - Control Flow Analysis (Controle de Análise de Fluxo) (incluindo Complexidade Ciclomática)
%
%CFA pode ser conduzida usando ferramentas ou manualmente sob varios niveis de
%abstracao (modulo, nó, etc) e é pelos seguintes razões:
%
%Garantir que o código é executado na sequencia correta
%Garantir que o código é bem estruturado
%Localizar qualquer código sintaticamente não atingivel
%Destacar partes do código (ex loops) onde o termino precisa ser considerado
%
%Isto pode resultar em representacoes diagramas e graficos do codigo sendo
%produzido.
%
%Interprocedural analysis is fairly straightforward in a language with only first-
%order functions. If we introduce higher-order functions, objects, or function
%pointers, then control flow and dataflow suddenly becomes intertwined. The
%task of control flow analysis is to approximate conservatively the control flow
%graph for such languages.
%\cite{Schwartzbach2008}


%\item \textit{}.
%Trace Flow Graph (TFG)
%  É usado para representar programas concorrentes, é baseado no modelo CFG e
%  extendido com informações adicionais nos vértices e nós para controle de
%  fluxo inter processo.


%\item \textit{Program Dependence Graph (PDG)}.
%PDG representa tanto controle quando dados dependencias juntos num grafo só.
%Foi desenvovido para apoiar otimizacoes necessarias para paralelismo. É um
%grafo firecionado, onde nós são as instruções, expressões ou regiões de nós, e
%arestas são controle ou dependencia de dados.
%Statement nodes represent instructions in the program. Predicate nodes test a con-
%ditional statement and have true and false edges to represent the choice taken on
%evaluation of the predicate. Region nodes group all nodes with the same control depen-
%dences together and order them into a hierarchy.



%\item \textit{Grafo acíclico dirigido}.
%  Grafo acíclico dirigido (DAG - Directed Acyclic Graph) é uma AST com
%    transformação suficiente para avitar duplocações de instruções, ela
%    permite cada nó possuir multiplos pais. Isto permite sub-arvores identicas
%    no grapho serem reutilizadas. Isso faz o DAG ser uma forma mais compacta
%    resultando em economia de memoria e processamento.

por exemplo gerar código removendo redundancias a fim de evitar computar
resultados já obtidos.


%Para um property checker, buscas potenciais
%vulnerabilidades de buffer overflow podem ser feitas como a checagem da
%propriedade "este prorama nao acessa um endereco fora dos limites de memoria
%alocados". Para o dominio de localizacao de bugs, ferramentas de seguranca
%adoram a nocao que desenvoovedores continuam recintando o mesmo metodo
%inseguro para resolver um problema, o que pode ser descrito como um idiome
%inseguro de programacao.
%Neste sentido, estas ferramentas estao talvez mais
%perto relacionadamente com verificadores de estilo - o que elas apontam nao
%necessariamente podem causar problemas de segurança, mas indicam que há razões
%para preocupar-se com problemas de segurança. 
%Ao longo do tempo estas
%ferramentas foram indicadas por terem alto indice de falso positivos porque
%pessoas tentavam interpretas a saida da ferramenta como uma lista de bugs ao
%inves de uma ajuda/apoio durante a revisao de codigo. 


\item \textit{Tabela de identificadores}.
  Tabela de identificadores (Identifiers Table) é uma tabela contendo todos os
    identificadores extraídos do código fonte.


%The authors state that optimizing a
%program in VDG form is simpler to implement, easier to express formally, and faster
%than equivalent CFG analysis. 
%An interesting property of the VDG is that is is implic-
%itly in SSA form: for every operator node, that node will have zero or more successors
%using its value.
%The original construction algorithm for the VDG begins from a CFG, where single-
%entry, single-exit (SESE) analysis is performed.


Na Data flow analysis é comum pensar no programa como um grafo: os nós são os
blocos elementares e as arestas descrevem como o controle pode passar de um
bloco elementar para outro. Podem existir muitas abordagens neste tipo de
análise, exemplos, equational approach, constraint based approach.

    regras de segurança no projeto do programa, tal como nos privilégios de
    ``root'' se o programa chama a função ``exec'' isto vai trazer implicações
    de segurança.

    Após as vulnerabilidades de segurança do programa serem
encontradas, os fabricantes normalmente lançam patches correspondentes, assim
você pode comparar o código com os patches para determinar a localização e as
causas da vulnerabilidade.

Usada para verificar propriedades de uma programa através de manipulação
algebrica do codigo fonte, sem requerer especificacao formal. Envolve checar
semantica de cada caminho atraves secoes do programa ou precedimentos.
Ferramentas sofisticadas dao expressoes para uma precisa relacao matematica
entre as entradas e saidas de uma secao particular do programa: Ele
efetivamente da a funcoes de transferir para esta secao do programa. Ele passa
pelo codigo, associado expressoes ao inves de valores para cada variavel.
Entao a logica sequencial é convertida em um conjunto de associacoes paralelas
que os valores de saida sao expresso em termos de valores de entrada - este
formato eh facil de interpretar. Ferramentas produzem saida para cada caminho
cosistindo de condicoes que causa o caminho ser executado, e o resultado da
execucao do caminho.

Se verificação de modelos realmente se encaixa na categoria de
análise de código fonte automática é discutível.  Na verdade, , na maioria das vezes em um formato específico
ferramenta. No entanto, uma vez que esta especificação foi estabelecida
nenhuma entrada humana adicional é necessário para realizar a análise.

  
%Esta técnica pode
%detectar: Overflow and underflow analysis, Rounding errors, Array bounds,
%Stack usage analysis.

%\section{Teoria da análise estática}
 Um tema presente
em todas as abordagens da análise de programa e consequentemente da análise de
código-fonte e da análise estática de código-fonte é que elas podem fornecer
apenas resultados aproximados \cite{Landi1992}.


%
% The expressive power of programming languages makes
% analysis even harder. Analysis routines must be explicitly
% developed to handle coding complexities, such as loops,
% conditional control flow, arrays, different variable types,
% interprocedural function calls, and aliasing. In practice no
% tool handles all possible code constructs.
% \cite{Black2007} 
%
%(falar de falso positivos / negativos)
%
%(Tools that are sound with respect to a counterexample are sometimes called
%complete in academic circles.)
%
%Existem diferentes técnicas de análise estática de código-fonte, que variam desde
%simples análises léxicas, análises de fluxo de dados até a análise de grafos de fluxo de con-
%trole. Ao apontar defeitos no código, uma ferramenta de análise estática pode ou não
%cometer erros quando analisando determinado trecho de código, dada a natureza do
%defeito ali presente (ou ausência de defeitos) e à técnica de análise empregada. Al-
%gumas técnicas são menos complexas e estão mais suscetíveis a erros do que outras
%(KRATKIEWICZ, 2005). Ainda assim, cada uma das técnicas de análise estática tem
%suas limitações (CUOQ; KIRCHNER; YAKOBOWSKI, 2012), o que justifica o uso de e
%a busca por diferentes técnicas de análise.
%
%Como definido em (BLACK et al., 2007), chama-se falso positivo, ou alarme
%falso, um trecho de código apontado como defeituoso por uma ferramenta de análise
%estática que, na verdade, não se trata de um defeito, ou seja, a ferramenta cometeu
%um erro ao apontar um trecho de código não defeituoso como defeituoso. Chama-se falso
%negativo um defeito no código fonte, que se enquadra na categoria de defeitos detectados
%pela ferramenta que analisa o código em questão, que não foi detectado pela ferramenta,
%ou seja, a ferramenta cometeu um erro ao não apontar um trecho de código defeituoso.
%Uma ferramenta ideal não cometeria erros, apontando todos os defeitos do código
%(mesmo que apenas em uma dada categoria) e nunca apontando um trecho de código
%correto como defeituoso (a ferramenta não comete falsos positivos nem falsos negativos).
%Trabalhos anteriores mostram que é impossível, mesmo em teoria, produzir tal ferramenta
%(BLACK et al., 2007) devido ao Teorema de Rice (RICE, 1953), que prova que para
%qualquer propriedade de software que não seja trivial, não existe algoritmo capaz de decidir
%se um programa qualquer possui tal propriedade, de modo que buscam-se aproximações
%para a solução de problemas ou tomadas de decisões relacionadas à análise estática. É
%possível, porém, produzir ferramentas capazes de cometer apenas um dos tipos de erro
%(falsos positivos ou falsos negativos). À análise estática que não gera falsos negativos, ou
%seja, aponta todos os defeitos no código, chamamos análise correta. À análise estática
%que não gera falsos positivos, ou seja, todos os possíveis defeitos apontados por ela são
%de fato trechos de código defeituosos, chama-se análise completa. Por fim, pode-se
%relacionar os termos com os Teoremas de Incompletude (GODEL et al., 1986), de modo
%que não é possível realizar uma análise completa e correta.

    
%Este formato é usado pela ferramenta Bunch e apresenta programas em um
%nível grosseiro de granularidade. Seus certices representam modulos do sistema
%e as arestas as dependencias entre eles.

    
%Foi
%desenvolvida para substituir o CFG como base de analises e transformacoes.

 Tais técnicas estão relacionadas à análise propriamente dita.


%Segundo o padrão ISO/IEC 25010 \cite{iso2011iec25010}, 
%uma métrica é a composição de procedimentos para a definição de 
%escalas e métodos para medição.


\cite{Chelf2008} uma patente ``SYSTEMS AND METHODS FOR PERFORMING STATIC
ANALYSIS ON SOURCE CODE'' descrevendo métodos e técnicas de análise estática
de código-fonte.

%\subsection{Ferramentas para cálculo de métricas de código-fonte}
%
%Estes diversos métodos e técnicas, formatos de representação interna estão
%presentes em diversas ferramentas de análise estática de código fonte, sejam
%proprietários, livres, comerciais ou não-comerciais. Uma das aplicações está o
%cálculo de métricas de código-fonte.
%
%(pendente)

\cite{Rutar2004} em seu artigo ``A Comparison of Bug Finding Tools for Java''
compara cinco ferramentas para bug finding em Java e discute as técnisas
utilizadas por cada uma e seu impacto nos resultados.

\cite{Kratkiewicz2005} em seu artigo ``Evaluating Static Analysis Tools for
Detecting Buffer Overflows in C Code'' avalia cinco ferramentas de análise
estática usando conjunto de testes para determinar seus pontos fortes e fracos
em detecção de uma variadade de falhas de buffer overflow em código C.

\cite{Okun2007} em seu artigo ``Effect of Static Analysis Tools on Software
Security: Preliminary Investigation'' avalia os efeitos de ferramentas de
segurança com objetivo de identificar se estas ferramentas melhoram de fato a
segurança de softwares.

\cite{Emanuelsson2008} em seu artigo ``A Comparative Study of Industrial
Static Analysis Tools'' avalia três ferramentas de análise estática da
indústria e se baseia em informações encontradas em outros artigos e manuais
destas ferramentas. Com objetivo principal de mapear as funcionalidades mais
significantes de ferramentas deste domínio que não são fornecidas por
compiladores normais.

\cite{Wedyan2009} em seu artigo ``The Effectiveness of Automated Static
Analysis Tools for Fault Detection and Refactoring Prediction'' avaliou três
ferramentas, propretária, software livre, desenvolvida por pesquisadores sobre
a efetividade de detectar falhas e predizer refatorações.

\cite{Mantere2009} em seu artigo ``Comparison of Static Code Analysis Tools''
compara três ferramentas da nálise estática de código-fonte, avalia a
performance através de experimentos e dá subsídios para ajudar na seleção de
uma ferramenta a ser tulizada em algum projeto.

\cite{Al2010} em seu artigo ``Comparing Four Static Analysis Tools for Java
Concurrency Bugs'' avalia quatro ferramentas de análise estática em relação à
capacidade de detectar bugs em programas concorrentes em Java e response se
ferramenta comerciais são melhores que ferramentas open source para isto.

\cite{Li2010} em seu artigo ``A Comparative Study on Software Vulnerability
Static Analysis Techniques and Tools'' compara sete diferentes ferramentas de
análise estática com foco em detecção de vulnerabilidades, comparando suas
caracteristicas através de um experimento.

\cite{Johns2011} em seu artigo ``Scanstud: A Methodology for Systematic,
Fine-grained Evaluation of Static Analysis Tools'' avaliou a qualidade de
ferramentas de análise estática de segurança a partir de uma série de
critérios.

\cite{Alemerien2013} em seu artigo ``Experimental Evaluation of Static Source
Code Analysis tools'' avalia 2 ferramentas de análise estática para cálculo de
métricas com objetivo de entender as diferenças de valores encontradas por
cada ferramenta e suas inconsistências.

\cite{Ataide2014} em seu artigo ``Static Code Analisys'' analisa os resultados
gerados por três ferramentas de análise estática que podem ser efecientemente
usadas por programadores para remover vulnerabilidades comuns em softwares.


%ASE primeira edição em 1990
%CSMR/SANER primeira edição em 1993
%ICSM/ICSME tem mais de 20 anos com primeira edição em 1993

valores referencia no trabalho de paulo:

ACC

Esses dados sinalizam que o monitoramento dessa métrica pode variar de acordo com a lin-
guagem de programação, domínio de aplicação ou mesmo de acordo com a maturidade (idade) do
projeto. Aqui, para a métrica ACC, vamos sugerir como referência de valores frequentes os projetos
com um certo equílibro dentre os demais, não necessariamente os com melhores valores. Para essa
escolha subjetiva, estamos considerando o conceito das métricas discutidos na Seção3.1.2.

Para projetos escritos em C, o código do Linux sinaliza ter valores frequentes não tão bons
quanto o do Android, mas factíveis de serem usados como uma referência para C: de 0 a 2,0: muito
frequente; de 2,1 a 7,0: frequente; de 7,1 a 13,0: pouco frequente; acima de 13,0: não frequente.

Para C++, o código do Firefox, mesmo com ACC não tão bom quanto o OpenOce, indica um
equilíbrio para usarmos seus valores frequentes: de 0 a 2,0: muito frequente; de 2,1 a 7,0: frequente;
de 7,1 a 15,0: pouco frequente; acima de 15,0: não frequente.

Para Java, o Open JDK8 pode ser
nossa referência ao observarmos os 14 projetos: de 0 a 1,0: muito frequente; 1,1 a 5,0: frequente; de
5,1 12,0: pouco frequente; acima de 12,0: não frequente.

\cite{Oliveira2013} define os intervalos 0 a 2, 2 a 7, e 7 a 15 como
excelente, bom e regular.

\cite{Meirelles2013} define como referência para Java os intervalos 0 a 1, 1 a
5, e 5 a 12 para os percentis 75, 90 e 95, respectivamente.

NPA

Os valores frequentes observados da métrica de atributos públicos das classes e variáveis públicas
dos módulos, de acordo com o apresentado na Tabela 4.27, podem ser agrupadas para as linguagens
C++ e Java. 

Para os projetos escritos em C sugerimos os valores de NPA para o código do GCC,
uma vez que não identicamos um projeto com valores equilibrados dentre os demais, segundo nossa
interpretação. O GCC apresenta os melhores valores, assim, para C temos: 0 muito frequente; até
2 frequente; de 3 a 5 pouco frequente; acima de 5 não frequente.

 Para os projetos escritos em C++
sugerimos: 0 muito frequente; 1 frequente; de 2 a 4 pouco frequente; acima de 4 não frequente. Por
m,

 para os projetos escritos em Java identicamos: 0 muito frequente; 1 frequente; 2 e 3 pouco
frequentes; acima de 3 não frequente.

ACCM

Da mesma forma como apresentado para a métrica ACC, no caso da métrica de complexidade
ciclomática, não conseguimos denir um conjunto de valores frequentes único para cada linguagem.
Dessa forma, com a mesma abordagem, observamos que os valores para a métrica ACCM para o
Linux, Firefox e Open JDK8 demonstram um equilíbrio entre os demais para podermos sugerir
seus valores como frequentes. Para C, de 0 a 3,6 muito frequente; de 3,1 a 5,3 frequente; de 5,4 a
7,0 pouco frequente, acima de 7,0 não frequente. Para projetos escritos em C++, de 0 a 2,0 muito
frequente; de 2,1 a 4,0 frequente; 4,1 a 6,0 pouco frequente; acima de 6 não frequente. Para os
projetos Java, de 0 a 2,8 muito frequente; de 2,9 a 4,4 frequente; de 4,5 a 6,0 pouco frequente;
acima de 6 não frequente.

Meirelles
(2013) definiu intervalos semelhantes para códigos em C, e valores um pouco reduzidos
para códigos em C++ e Java (0 a 2, 2 a 4, e 4 a 6 para os percentis 75, 90 e 95 respecti-
vamente).

Reforçando a importância dessa métrica em uma análise estática de código, Oliveira
(2013) define a mesma com um peso adicional em relação a outras métricas em seu estudo.
Valores de referência definidos para esse estudo foram 1 a 3, 3 a 5, e 5 a 7, para excelente,
bom e regular, respectivamente.

O valor teórico ideal para essa métrica é quando
o software tem apenas um caminho de execução, com complexidade ciclomática
igual a 1.

RFC

Para a métrica RFC, avaliamos que podemos interpretar os valores frequentes vericando os
percentis 75, 90 e 95, apesar da mediana poder ser considerada um ponto de corte ao observamos os
dados da Tabela 4.31. Como refutamos nossa hipótese H3 por conta da variação entre os valores dessa
métrica entre os projetos, ou seja, não conseguimos indicar valores únicos, mesmo agrupando por
linguagem de programação, sugerimos observar os valores da métrica RFC dos projetos FreeBSD,
Chrome e Open JDK8. Para os projetos escritos em C, sugerimos: de 0 a 54 muito frequente; de 55 a
152 frequente; de 153 a 271 pouco frequente; acima de 272 não frequente. Para os os projetos escritos
em C++, temos: de 0 a 29 muito frequente; de 30 a 64 frequente; de 65 a 102 pouco frequente;
acima de 102 não frequente. Ainda, para os projetos escritos em Java, indicamos: de 0 a 9 muito
frequente; de 10 a 26 frequente; de 27 a 59 pouco frequente; acima de 59 não frequente.

Meirelles (2013) define como bons intervalos para projetos Java valores de 0 a 9,
10 a 26, e 27 a 59, para os percentis 75, 90 e 95, respectivamente. Os valores na análise
da API obtidos neste trabalho estão bem acima desse valor, estando em seu percentil 75
um valor perto de 30, que seria no máximo regular nessa escala.

Baseando-se em todas essas observações, consideramos os seguintes intervalos:
• Valores abaixo 31 são frequentes para API Android. Para os aplicativos do sistema,
existe uma grande variância de valores, porém estando em sua grande maioria abaixo
de 38 para o percentil 75.
• RFC chegou a 130 em aplicativos nativos, porém no geral não alcançam o valor 85.
Esse mesmo valor é o limite para a API do sistema.
• Valores acima de 85 são valores considerados altos para a métrica RFC, e são pouco
frequentes nos dados analisados. Valores acima de 140 não são frequentes.

1994). Uma classe com alto valor de RFC
pode ser uma classe com um número muito grande de métodos, e/ou uma classe bastante
dependente de outra(s) classe(s). Um valor alto de RFC pode indicar baixa coesão e alto
acoplamento. Valores mais próximos a zero são os ideais teóricos dessa métrica.

AMLOC

Baseado nos valores observados no código do Linux, para um código escrito em C: de 0 a
15,6 muito frequente; de 15,7 a 25,5 frequente; de 25,6 a 39,3 pouco frequente; acima de 39,3 não
frequente. Para os projetos em C++, de acordo com valores do Firefox: de 0 a 8,0 muito frequente;
de 9 a 19,5 frequente; 19,6 a 37 pouco frequente; acima de 37 não frequente. Por m, para os projetos
escritos em Java, conforme observado no código do Open JDK8, temos para a AMLOC: de 0 a 8,3
muito frequente; de 8,4 a 18 frequente; de 19 a 34 pouco frequente; acima de 34 não frequente.

Intervalos encontrados:
• Valores abaixo de 14 são muito frequentes nos aplicativos e na API;
• Enquanto no sistema os valores para o percentil 90 estão abaixo de 31, nos aplicativos
eles alcançam esse valor em poucos casos, ficando em sua maioria abaixo de 25;
• Valores acima de 31 são pouco frequentes em ambos os casos;

 O valor de AMLOC
deve ser o menor possível, pois métodos grandes ``abrem espaço'' para
problemas de complexidade excessiva.

DIT

Nossa hipótese H3 é positiva para a métrica DIT. Conseguimos sugerir valores frequentes para
os projetos escritos em C++ e Java, observamos os valores dos percentis na Tabela 4.15. Para os
projetos C++ indicamos: 0 e 1 muito frequente; 2 e 3 frequente; 4 pouco frequente; acima de 4 não
frequente. No caso dos projetos escritos em Java, interpretamos: de 0 a 2 muito frequente; 3 e 4
frequente; 5 e 6 pouco frequente; acima de 6 não frequente.

, e caso haja herança múltipla, DIT mede a distância máxima até o nó raiz da ár-
vore de herança (CHIDAMBER; KEMERER, 1994). Se ela não herda nada, tem DIT igual
a 0

Os valores são em geral baixos, chegando a no máximo 4 até o percentil
95 em todos as versões da API, demonstrando valores bem menores dos que os intervalos
definidos por Meirelles (2013) para Java, que chegam até 2, 4 e 6 para os percentis 75, 90
e 95, respectivamente. Oliveira (2013) utiliza os mesmos intervalos para excelente, bom,
e regular, respectivamente. Ferreira et al. (2009) não define intervalos para essa métrica,
mas indica um valor 2 como referência.

Sobre árvore de herança, consideramos os seguintes intervalos para o Android:
• DIT até 1 e NOC igual a 0 são valores muito frequentes em todas as amostras.
• DIT até 2 e NOC igual a 1 são valores frequentes em todas as amostras.
• DIT até 4 e NOC igual a 2 são valores pouco frequentes para a API, mas para seus
aplicativos os valores de DIT no percentil 95 permanecem no número 2.

LCOM4

Para os projetos escritos em C temos: de 0 a 5 muito frequente; de 6 a 12 frequente; de 12 a 20
pouco frequente; acima de 20 não frequente. Se compararmos com os valores da LCOM4 no código
do Linux, ca evidenciado algo muitas vezes dito por programadores do
Kernel
sobre o FreeBSD
ter um código mais organizado e legível que o Linux. Certamente, a maior ausência de coesão nas
funções dos módulos do Linux reete nesse tipo de constatação. Baseado no código do Chrome, os
valores da LCOM para C++ são: de 0 a 5 muito frequente; de 6 a 10 frequente; de 10 a 14 pouco
frequente; acima de 14 não frequente. Para os projetos escritos em Java sugerimos: de 0 a 3 muito
frequente; de 4 a 7 frequente; de 8 a 12 pouco frequente; acima de 12 não frequente.

. O valor ideal teórico de LCOM4 é 1, que representa a maior coesão possível,
e valores maiores que isso podem indicar que a classe está com muita responsabilidade,
tentando alcançar muitos propósitos distintos.

A Tabela 12 mostra que em todas as versões da API Android o valor muito fre-
quente é 3. De 3 a 7 são valores frequentes, e de 7 a 12 pouco frequentes. Valores acima
de 12 não são frequentes no sistema.

Meirelles (2013) define para projetos Java intervalos de 0 a 3, 4 a 7, e 8 a 12 para
os percentis 75, 90 e 95, respectivamente. Os resultados encontrados aqui acompanharam
muito bem esses intervalos.

A Tabela 13 mostra que os intervalos 0 a 4, 4 a 7, e 7 a
12 são válidos para a grande maioria dos aplicativos. Os resultados são muito parecidos
com os valores para a API, sendo que a métrica só aumenta em 1 no percentil 75.

LOC

Partindo da mediana como primeiro ponto de corte, para os projetos escritos em C temos: de
0 a 79 muito frequente; de 80 a 324 frequente; de 325 a 877 pouco frequente; acima de 877 não
frequente. Para os projetos C++ sugerimos: de 0 a 31 muito frequente; de 32 a 84 frequente, de
85 a 207 pouco frequente; acima de 207 não frequente. Para projetos escritos em Java indicamos:
de 0 a 33 muito frequente; de 34 a 87 frequente; de 88 a 200 pouco frequente; acima de 200 não
frequente.

 Para efetuar comparações entre sistemas usando LOC, é necessário
que ambos tenham sido feitos na mesma linguagem de programação e que o estilo esteja
normalizado (Jones, 1991).

Os intervalos sugeridos para o LOC de uma classe (Java e C++)
são: até 70 (bom); entre 70 e 130 (regular); de 130 em diante (ruim).

ANPM

 Seu valor mínimo é zero
e não existe um limite máximo para o seu resultado, mas um número alto de parâmetros
pode indicar que um método pode ter mais uma responsabilidade, ou seja, mais de uma
função \cite{Jagdish1997}.

Dessa forma, para projetos escritos em C
sinalizamos: de 0 a 3,0 muito frequente; de 3,1 a 4,0 frequente; 4,1 a 5,0 pouco frequente; acima de
5,0 não frequente. Aos projetos escritos em C++ podemos indicar: de 0 a 2,0 muito frequente; 2,1
a 3,0 frequente; 3,1 a 5,0 pouco frequente; acima de 5,0 não frequente. Para os projetos em Java
temos: de 0 a 1,5 muito frequente; de 1,6 a 2,0 frequente; de 2,1 a 4,0 pouco frequente; acima de
4,0 não frequente.

NOA

Seguindo nossa abordagem para a maioria das métricas apresentadas até aqui, sugerimos os
valores frequentes para a métrica NOA vericando os percetins 75, 90 e 95. Para os projetos escritos
em C, baseado nos valores do número de variáveis dos módulos do FreeBSD, temos: de 0 a 9 muito
frequente; de 10 a 29 frequente; de 30 a 57 pouco frequente; acima de 57 não frequente. Baseado
nos valores de NOA para o Chrome, sugerimos: de 0 a 4 muito frequente; de 5 a 8 frequente; de 9
a 13 pouco frequente; acima de 13 não frequente. Para os projetos escritos em Java, de acordo com
os valores de NOA para o Eclipse, indicamos: de 0 a 3 muito frequente; de 4 a 8 frequente; de 9 a
12 pouco frequente.

 Seu valor mínimo é zero e não
existe um limite máximo para o seu resultado. Uma classe com muitos atributos
pode indicar que ela tem muitas responsabilidades e apresentar pouca coesão,
i.e., deve estar tratando de vários assuntos diferentes.

NOM

A variação dos valores, mesmo nos projetos da mesma linguagem, não nos levou a indicarmos
valores frequentes únicos por linguagem de programação, por exemplo. Dessa forma, nossa hipótese
H3, como para a maioria das métricas, foi refutada para a métrica NOM. Assim, indicamos os
projetos FreeBSD, Chrome e OpenJDK8 como fonte dos valores frequentes, segundo nossa inter-
pretação e abordagem. Para os projetos escritos em C temos: de 0 a 13 muito frequente; de 14 a
29 frequente; de 30 a 48 pouco frequente; acima de 48 não frequente. Para os projetos escritos em
C++ indicamos: de 0 a 10 muito frequente; de 11 a 17 frequente; 18 a 26 pouco frequente; acima
de 26 não frequente.

Essa métrica é usada para ajudar a identicar o potencial de reúso de uma
classe. Em geral, as classes com um grande número de métodos são mais difíceis
de serem reutilizadas, pois elas são propensas a serem menos coesas (Lorenz e
Kidd, 1994).

CBO

De qualquer forma, para os projetos escritos em C, indicamos os valores de
CBO baseado também no código do Linux  argumentamos que também para CBO seus valores
têm um equílibrio para servir de referência aos demais escritos em C: de 0 a 5 muito frequente, de
6 a 9 frequente; de 9 a 12 pouco frequente; acima de 12 não frequente.
Para os projetos escritos em C++ sugerimos os valores de CBO do Chrome como uma possível
referência dos valores frequentes: de 0 a 3 muito frequente; 4 e 5 frequente; 6 e 7 pouco frequente;
acima de 7 não frequente. No caso dos projetos Java, já observamos no gráco da Figura 4.5(b) que
os valores de CBO das classes do Eclipse e do Open JDK são bem próximos. Olhando a Tabela 4.13,
vericamos que os percentis 75, 90 e 95 têm os mesmos valores para esses projetos: de 0 a 3 muito
frequente; de 4 a 6 frequente; de 7 a 9 pouco frequente; acima de 9 não frequente. Portando, em
relação à nossa hipótese H3, apenas conseguimos agrupar valores únicos para a métrica CBO
em códigos escritos em Java. Nos casos de C e C++ apenas apresentamos um dos projetos como
uma referência, de acordo com nossa interpretação do Linux e do Chrome para com os respectivos
projetos da mesma linguagem, ou seja, para C e C++ nossa hipótese H3 é negada.

NOC

Para o NOC, conseguimos sugerir valores frequentes para os projetos escritos em C++ e Java,
observamos os valores dos percentis na Tabela 4.15. Assim, conrmando nossa hipótese H3, sobre
valores frequente por linguagem de programação. Para os projetos C++ indicamos: 0 muito fre-
quente; 1 frequente; 2 pouco frequente; acima de 2 não frequente. No caso dos projetos escritos em
Java, interpretamos da seguinte forma: de 0 muito frequente; 1 e 2 frequente; 3 pouco frequente;
acima de 3 não frequente.

NPM

Nossa terceira hipótese não é correspondida para os valores de NPM, conforme observamos nos
valores reportados na Tabela 4.29. Dessa forma, sugerimos vericar os valores de NPM do GCC,
Chrome e Open JDK8. Para os projetos escritos em C temos: de 0 a 2 muito frequente; de 3 a 5
frequente; de 6 a 14 pouco frequente; acima de 14 não frequente. No caso dos projetos escritos em
C++ indicamos: de 0 a 3 muito frequente; de 4 a 7 frequente; de 8 a 13 pouco frequente; acima de
13 não frequente.

SC

No caso da métrica de complexidade estrutural, mais uma vez refutamos a hipótese H3, como
podemos observar nos valores reportados na Tabela 4.33. Novamente, avaliamos os projetos FreeBSD,
Chrome e OpenJDK como possíveis referências para C, C++ e Java, respectivamente. Assim, para
a métrica SC em projetos escritos em C, temos: de 0 a 18 muito frequente; de 19 a 77 frequente; de
78 a 168 pouco frequente; acima de 168 não frequente. Para os projetos em C++, indicamos: de 0 a
12 muito frequente; de 13 a 28 frequente; de 29 a 51 pouco frequente; acima de 51 não frequente. Por
último, para os projetos escritos em Java, sugerimos: de 0 a 6 muito frequente; de 7 a 21 frequente;
de 22 a 45 pouco frequente; acima de 45 não frequente.


%\citeonline{Ronaldo2015} realiza um estudo para o monitoramento de métricas de
%código fonte na API do sistema operacional Android, avalia sua evolução,
%estuda as semelhanças da API com outros aplicativos do sistema Android.
%Apresenta uma proposta de cálculo de similaridade entre aplicativos e a API
%Android. Encontra valores de referências para métricas do sistema Android e
%propõe um método de cálculo da distância entre aplicativos e os valores de
%referência.

%\citeonline{Almeida2010} realizam um estudo mapeando boas práticas de
%programação em valores de métricas de código-fonte, estas boas práticas são
%baseadas nos trabalhos de \cite{Martin2012} e \cite{Beck2007} sobre {\it Clean
%Code}, onde sugerem práticas de desenvolvimento úteis para que um código
%tenham expressividade, simplicidade e flexibilidade. Neste trabalho os autores
%identificam melhorias de implementação através do uso de valores de métricas e
%oferecem aos desenvolvedores uma maneira de pensarem em melhorias para os seus
%códigos.
