\section{Software como cidadão de primeira classe na ciência}

JOSS, Papers executáveis, pesquisas reproduzíveis, etc, dar credibiidade ao
pesquisador cientista desenvolvedor de software ... papel do software na
reprodutibilidade, etc...

Em ciência da computação, particularmente em engenharia de software, tem-se
notado um aumento constante no número de novos softwares acadêmicos
\cite{allen2017engineering}.


%A situação com software é amplamente análoga (mas não identica) ao de dados
%das publicações; de fato, todo dado é processado por softwares de alguma forma
%(Borgman et al., 2012).

%\item The GeoScience paper of the future initiative \cite{OntoSoft2016}\footnote{\url{http://www.scientificpaperofthefuture.org/gpf/what-is-a-gpf}}
%Possui um conjunto de requerimentos para softwares serem incluidos em
%papers.  Focando mais no paper em sí do que no software.

%Linked Open Science—Communicating, Sharing and Evaluating
%Data, Methods and Results for Executable Papers.
%Linked Open Science is an approach to solve challenges of an executable paper. It is a combination of four “silver
%bullets”: 1) publication of scientific data, metadata, results, and provenance information using Linked Data principles,
%2) open source and web-based environments for executing, validating and exploring research, 3) Cloud Computing
%for efficient and distributed computing, and 4) Creative Commons for the legal infrastructure. We will use a realistic
%scientific research setting related to research on deforestation of the Brazilian Amazon rainforest to provide scenarios
%to illustrate the application of Linked Open Science \cite{Kauppinen2011}.

%Diversas maneiras de inventivar citação formal entre artefatos digitais,
%software por exemplo, tem surgido, dentre elas uma iniciativa interessante
%é o Journal of Open Source Software (JOSS) é um livre a open-access jornal para
%publicação de artigos descrevendo software acadêmico. Ele tem dois objetivos
%principais, melhorar a qualidade dos softwares submetidos e prover mecanismos
%para pesquisadores desenvolvedores de software acadêmico receber crédito pelos
%seus softwares. Enquanto pensado para trabalhar dentro do atual sistema de
%mérito da ciência, JOSS visa a escassez de recompensas para contribuições
%importantes para a ciência realizadas em forma de software. JOSS publica
%artigos que encapsulam sabedoria contida no software ele mesmo, e seu rigoroso
%revisão em pares mirado nos componentes do software: funcionalidade,
%documentação, testes, integração contínua, e a licença. Um artigo JOSS contém
%um resumo descrevendo o objetivo e funcionalidades do software, referencias, e
%um link para o software archive.  O artigo é um ponto de entrada para
%submissçao que engloba o conjunto completo de artefatos de software. Artigos
%aceitos no JOSS recebem um digital object identifier (DOI), te seus metadados
%depositados no Crossref, e o artigo pode começar a colecionar citações e ser
%indexados em serviços como Google Scholar e outros. No seu primeiro ano,
%iniciado em Maio de 2016, JOSS publicou 111 artigos, com mais de 40 artigos
%adicionais sob revisão \cite{smith2017journal}.

%\item UK RSE \cite{ukrse2013}\footnote{\url{http://rse.ac.uk/who}}
%Conscientização sobre a importância e o papel do {\it Research Software
%Engineer} através de comunicação e suporta institucional.

%FORCE11 Software Citation principles \cite{smith2016software}\footnote{\url{https://www.force11.org/software-citation-principles}}
%Enfatiza persistencia e claridade e diz que ``Software deve ser considerado
%um produto legítimo de pesquisas e devem ser possível de serem citados''.

%Reproducibility verification is essential to the practice of the scientific method.
%Researchers report their findings, which are strengthened as other independent groups
%in the scientific community share similar outcomes. In the many scientific fields
%where software has become a fundamental tool for capturing and analyzing data, this
%requirement of reproducibility implies that reliable and comprehensive software platforms
%and tools should be made available to the scientific community. The tools will empower
%them and the public to verify, through practice, the reproducibility of observations that
%are reported in the scientific literature. Medical image analysis is one of the fields in
%which the use of computational resources, both software and hardware, are an essential
%platform for performing experimental work. In this arena, the introduction of the Insight
%Toolkit (ITK) in 1999 has transformed the field and facilitates its progress by accelerating
%the rate at which algorithmic implementations are developed, tested, disseminated and
%improved. By building on the efficiency and quality of open source methodologies, ITK has
%provided the medical image community with an effective platform on which to build a daily
%workflow that incorporates the true scientific practices of reproducibility verification. This
%article describes the multiple tools, methodologies, and practices that the ITK community
%has adopted, refined, and followed during the past decade, in order to become one of the
%research communities with the most modern reproducibility verification infrastructure. For
%example, 207 contributors have created over 2400 unit tests that provide over 84% code
%line test coverage. The Insight Journal, an open publication journal associated with the
%toolkit, has seen over 360,000 publication downloads. The median normalized closeness
%centrality, a measure of knowledge flow, resulting from the distributed peer code review
%system was high, 0.46 \cite{McCormick2014}.

%Among empirical software engineering studies, those based on data re-
%trieved from development repositories (such as those of source code management,
%issue tracking or communication systems) are specially suitable for reproduction.
%However their reproducibility status can vary a lot, from easy to almost impossible
%to reproduce. This paper explores which elements can be considered to characterize
%the reproducibility of a study in this area, and how they can be analyzed to better
%understand the type of reproduction studies they enable or obstruct. One of the
%main results of this exploration is the need of a systematic approach to asses the
%reproducibility of a study, due to the complexity of the processes usually involved,
%and the many details to be taken into account. To address this need, a methodology
%for assessing the reproducibility of studies is also presented and discussed, as a tool to
%help to raise awareness about research reproducibility in this field. The application
%of the methodology in practice has shown how, even for papers aimed to be
%reproducible, a systematic analysis raises important aspects that render reproduction
%difficult or impossible. We also show how, by identifying elements and attributes
%related to reproducibility, it can be better understood which kind of reproduction
%can be done for a specific study, given the description of datasets, methodologies and
%parameters it uses \cite{gonzalez2012reproducibility}.

%Science rests on peer review and the wide-spread dissemination of
%knowledge. Software engineering research will advance further and
%faster if the sharing of data and tools were easier and more wide-
%spread. Pragmatic concerns hinder the realization of this ideal: the
%time and effort required and the risk of being scooped. We examine
%the costs and benefits of facilitating sharing in our field in an effort
%to help the community understand what problems exist and find
%a solution. We examine how other fields, such as medicine and
%physics, handle sharing, describe the value of sharing for replication
%and innovation, and address practical concerns such as standards
%and warehousing. To launch what we hope will become an ongoing
%discussion of solutions in our community, we present some ways
%forward that mitigate the risk of sharing — partial sharing, registry,
%escrow, and market \cite{barr2010shoulders}.

%At various machine learning conferences, at
%various times, there have been discussions
%arising from the inability to replicate the
%experimental results published in a paper.
%There seems to be a wide spread view that we
%need to do something to address this prob-
%lem, as it is essential to the advancement
%of our field. The most compelling argument
%would seem to be that reproducibility of ex-
%perimental results is the hallmark of science.
%Therefore, given that most of us regard ma-
%chine learning as a scientific discipline, being
%able to replicate experiments is paramount.
%I want to challenge this view by separating
%the notion of reproducibility, a generally de-
%sirable property, from replicability, its poor
%cousin. I claim there are important differ-
%ences between the two. Reproducibility re-
%quires changes; replicability avoids them. Al-
%though reproducibility is desirable, I contend
%that the impoverished version, replicability,
%is one not worth having \cite{drummond2009replicability}.

%Abstract—This paper is the result of reviewing all papers
%published in the proceedings of the former International
%Workshop on Mining Software Repositories (MSR) (2004-2006)
%and now Working Conference on MSR (2007-2009). We have
%analyzed the papers that contained any experimental analysis
%of software projects for their potentiality of being replicated.
%In this regard, three main issues have been addressed: i) the
%public availability of the data used as case study, ii) the public
%availability of the processed dataset used by researchers and iii)
%the public availability of the tools and scripts. A total number of
%171 papers have been analyzed from the six workshops/working
%conferences up to date. Results show that MSR authors use
%in general publicly available data sources, mainly from free
%software repositories, but that the amount of publicly available
%processed datasets is very low. Regarding tools and scripts, for
%a majority of papers we have not been able to find any tool,
%even for papers where the authors explicitly state that they have
%built one. Lessons learned from the experience of reviewing the
%whole MSR literature and some potential solutions to lower the
%barriers of replicability are finally presented and discussed
%\cite{robles2010replicating}.

%A survey of controlled experiments in software engineering
%Among
%the 20 replications, five can be considered as close replica-
%tions in the terminology of Lindsay and Ehrenberg [31], i.e.,
%one attempts to retain, as much as is possible, most of the
%known conditions of the original experiment.

%Replication of empirical studies in software engineering: Preliminary findings from a systematic mapping study
%The number of replications grew in the last few years, but the
%absolute number of replications is still very small, in particular
%considering the breadth of topics in software engineering. Incentive
%to perform external replications and better standards to report
%empirical studies and their replications are still needed.

%Um fator em favor da aceitação dos conceitos da EBSE tem sido a crescente
%reconhecimento que os resultados de estudos empíricos individuais são frequentemente
%inconclusivos, e estes tipo de estudos são difícels de replicar com sucesso
%\cite{sjoberg2005survey}.

%\item Reproducibility manifesto \cite{Barba2012}\footnote{\url{http://lorenabarba.com/gallery/reproducibility-pi-manifesto}}
%Inclui termos para fazer softwares reusáveis por outros. Foco em
%reprodutibilidade, deixando sustentabilidade de software fora de questão.
